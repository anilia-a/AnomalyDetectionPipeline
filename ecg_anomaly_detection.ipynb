{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Anomaly Detection with Counterfactual Explanations\n",
    "\n",
    "3 layer explainable AI pipeline for ECG anomaly detection:\n",
    "\n",
    "1. Detection layer - Ensemble of Isolation Forest and OCSVM\n",
    "2. Counterfactual layer - uses nearest neighbour approach to generate counterfactual explanations\n",
    "3. Explanation layer: LLM (open ai) powered clinical translations of the counterfactual explanations\n",
    "\n",
    "## Dataset\n",
    "ECG200 dataset from the UCR Time Series Archive:\n",
    "- 200 ECG samples (100 train, 100 test)\n",
    "- Class 1 = Normal heartbeats\n",
    "- Class -1 = Myocardial infarction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wfdb numpy pandas scipy scikit-learn matplotlib seaborn openai tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from scipy import signal\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    precision_score, recall_score, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    # Data \n",
    "    sampling_rate: int = 100    # Hz (ECG200 is variable) (PTBXL uses 100Hz/500Hz)\n",
    "    signal_length: int = 96 # ECG200 has 96 timesteps per sample\n",
    "    n_leads: int = 1    # ECG200 is single lead\n",
    "\n",
    "    # Preprocessing \n",
    "    lowcut: float = 0.5   # high pass filter cutoff\n",
    "    highcut: float = 40.0 # low pass filter cutoff\n",
    "    filter_order: int = 4\n",
    "\n",
    "    # Anomaly detection \n",
    "    contamination: float = 0.1  # expected proportion of anomalies\n",
    "    if_n_estimators: int = 100  # Isolation Forest trees\n",
    "    svm_kernel: str = 'rbf' # OCSVM kernel\n",
    "    svm_nu: float = 0.1 # OCSVM nu parameter\n",
    "    svm_gamma: str = 'scale'    # OCSM gamma\n",
    "\n",
    "    # Counterfactual \n",
    "    n_neighbors: int = 5    # 5 nearest neighbours\n",
    "    distance_metric: str = 'euclidean'\n",
    "\n",
    "    # LLM \n",
    "    llm_backend: str = 'openai'                    \n",
    "    llm_model: str = 'gpt-4o-mini'  # model to use\n",
    "    llm_temperature: float = 0.2    # lower for more consistent outputs\n",
    "    llm_max_tokens: int = 512\n",
    "    llm_api_key_env: str = 'OPENAI_KEY'       \n",
    "    \n",
    "    # llm_local_model: str = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "    # llm_local_cache_dir: str = '/tmp/hf_cache'\n",
    "    \n",
    "    data_dir: str = './data'\n",
    "    output_dir: str = './outputs'\n",
    "\n",
    "config = PipelineConfig()\n",
    "print(f\"Pipeline configured with: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataLoader:\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    # ECG200 contains 200 ECG samples:\n",
    "        #   Class 1 = Normal \n",
    "        #   Class -1 = Myocardial infarction\n",
    "    def load_ecg200(self, train_path: str, test_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        # In ECG200 first column is label, rest is time series \n",
    "        train_data = np.loadtxt(train_path)\n",
    "        test_data = np.loadtxt(test_path)\n",
    "        \n",
    "        y_train = train_data[:, 0]\n",
    "        X_train = train_data[:, 1:]\n",
    "        y_test = test_data[:, 0]\n",
    "        X_test = test_data[:, 1:]\n",
    "        \n",
    "        # convert the labels 1 -> 0 (normal) and -1 -> 1 (anomaly)\n",
    "        y_train = np.where(y_train == 1, 0, 1)\n",
    "        y_test = np.where(y_test == 1, 0, 1)\n",
    "        \n",
    "        print(f\"ECG200 loaded:\")\n",
    "        print(f\"  Training: {X_train.shape[0]} samples, {X_train.shape[1]} timesteps\")\n",
    "        print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "        print(f\"  Normal (train): {np.sum(y_train == 0)}, Anomaly (train): {np.sum(y_train == 1)}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass filtering, normalisation and quality checks.\n",
    "class ECGPreprocessor:\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    # Butterworth bandpass filter to remove baseline wander and high frequency noise\n",
    "    def bandpass_filter(self, X: np.ndarray) -> np.ndarray:\n",
    "        # High pass (0.5 Hz): removes baseline wander from respiration\n",
    "        # Low-pass (40 Hz): removes muscle noise and powerline interference\n",
    "\n",
    "        nyquist = self.config.sampling_rate / 2\n",
    "        low = self.config.lowcut / nyquist\n",
    "        high = self.config.highcut / nyquist\n",
    "        \n",
    "        if high >= 1.0:\n",
    "            high = 0.99\n",
    "        if low <= 0.0:\n",
    "            low = 0.01\n",
    "            \n",
    "        b, a = signal.butter(self.config.filter_order, [low, high], btype='band')\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            return signal.filtfilt(b, a, X)\n",
    "        elif X.ndim == 2:\n",
    "            return np.array([signal.filtfilt(b, a, x) for x in X])\n",
    "        else:\n",
    "            filtered = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    filtered[i, :, j] = signal.filtfilt(b, a, X[i, :, j])\n",
    "            return filtered\n",
    "    \n",
    "    # z score normalisation\n",
    "    def normalize(self, X: np.ndarray, fit: bool = True) -> np.ndarray:\n",
    "        if X.ndim == 1:\n",
    "            return (X - np.mean(X)) / (np.std(X) + 1e-8)\n",
    "        elif X.ndim == 2:\n",
    "            return np.array([(x - np.mean(x)) / (np.std(x) + 1e-8) for x in X])\n",
    "        else:\n",
    "            normalized = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    sig = X[i, :, j]\n",
    "                    normalized[i, :, j] = (sig - np.mean(sig)) / (np.std(sig) + 1e-8)\n",
    "            return normalized\n",
    "    \n",
    "    def preprocess(self, X: np.ndarray, apply_filter: bool = True) -> np.ndarray:\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        if apply_filter:\n",
    "            X_processed = self.bandpass_filter(X_processed)\n",
    "        \n",
    "        X_processed = self.normalize(X_processed)\n",
    "        \n",
    "        return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ECG200 data\n",
    "loader = ECGDataLoader(config)\n",
    "preprocessor = ECGPreprocessor(config)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = loader.load_ecg200(\n",
    "     'data/ECG200/ECG200_TRAIN.txt',\n",
    "     'data/ECG200/ECG200_TEST.txt'\n",
    ")\n",
    "\n",
    "# preprocess\n",
    "X_train = preprocessor.preprocess(X_train_raw)\n",
    "X_test = preprocessor.preprocess(X_test_raw)\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Anomaly ratio (train): {np.mean(y_train):.2%}\")\n",
    "print(f\"Anomaly ratio (test): {np.mean(y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise Sample ECGs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "normal_idx = np.where(y_train == 0)[0][:2]\n",
    "anomaly_idx = np.where(y_train == 1)[0][:2]\n",
    "\n",
    "for i, idx in enumerate(normal_idx):\n",
    "    axes[0, i].plot(X_train[idx], 'b-', linewidth=0.8)\n",
    "    axes[0, i].set_title(f'Normal ECG (Sample {idx})', fontsize=12)\n",
    "    axes[0, i].set_xlabel('Time (samples)')\n",
    "    axes[0, i].set_ylabel('Amplitude (normalized)')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "for i, idx in enumerate(anomaly_idx):\n",
    "    axes[1, i].plot(X_train[idx], 'r-', linewidth=0.8)\n",
    "    axes[1, i].set_title(f'Anomalous ECG (Sample {idx})', fontsize=12)\n",
    "    axes[1, i].set_xlabel('Time (samples)')\n",
    "    axes[1, i].set_ylabel('Amplitude (normalized)')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ecg_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract clinically relevant features from the ECGs\n",
    "- Statistical features\n",
    "- Temporal/morphological features\n",
    "- Frequency domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGFeatureExtractor:\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.feature_names = []\n",
    "    \n",
    "    # statistical features\n",
    "    def extract_statistical_features(self, x: np.ndarray) -> Dict[str, float]:\n",
    "        return {\n",
    "            'mean': np.mean(x),\n",
    "            'std': np.std(x),\n",
    "            'var': np.var(x),\n",
    "            'min': np.min(x),\n",
    "            'max': np.max(x),\n",
    "            'range': np.max(x) - np.min(x),\n",
    "            'median': np.median(x),\n",
    "            'skewness': skew(x),\n",
    "            'kurtosis': kurtosis(x),\n",
    "            'rms': np.sqrt(np.mean(x**2)),\n",
    "            'zero_crossings': np.sum(np.diff(np.signbit(x)))\n",
    "        }\n",
    "    \n",
    "    # temporal/morphological features \n",
    "    def extract_temporal_features(self, x: np.ndarray) -> Dict[str, float]:\n",
    "        threshold = np.mean(x) + 0.5 * np.std(x)\n",
    "        peaks, properties = signal.find_peaks(x, height=threshold, distance=10)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        if len(peaks) > 1:\n",
    "            rr_intervals = np.diff(peaks)\n",
    "            features['rr_mean'] = np.mean(rr_intervals)\n",
    "            features['rr_std'] = np.std(rr_intervals)\n",
    "            features['rr_min'] = np.min(rr_intervals)\n",
    "            features['rr_max'] = np.max(rr_intervals)\n",
    "            features['hr_estimate'] = 60 * self.config.sampling_rate / np.mean(rr_intervals)\n",
    "            features['hrv'] = np.std(rr_intervals) / (np.mean(rr_intervals) + 1e-8)\n",
    "        else:\n",
    "            features['rr_mean'] = 0\n",
    "            features['rr_std'] = 0\n",
    "            features['rr_min'] = 0\n",
    "            features['rr_max'] = 0\n",
    "            features['hr_estimate'] = 0\n",
    "            features['hrv'] = 0\n",
    "        \n",
    "        features['n_peaks'] = len(peaks)\n",
    "        if len(peaks) > 0:\n",
    "            peak_heights = x[peaks]\n",
    "            features['peak_height_mean'] = np.mean(peak_heights)\n",
    "            features['peak_height_std'] = np.std(peak_heights)\n",
    "        else:\n",
    "            features['peak_height_mean'] = 0\n",
    "            features['peak_height_std'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # frequency domain features using fft\n",
    "    def extract_frequency_features(self, x: np.ndarray) -> Dict[str, float]:\n",
    "        fft_vals = np.abs(np.fft.rfft(x))\n",
    "        fft_freq = np.fft.rfftfreq(len(x), d=1/self.config.sampling_rate)\n",
    "        \n",
    "        total_power = np.sum(fft_vals**2) + 1e-8\n",
    "        \n",
    "        vlf_mask = (fft_freq >= 0) & (fft_freq < 0.04)\n",
    "        lf_mask = (fft_freq >= 0.04) & (fft_freq < 0.15)\n",
    "        hf_mask = (fft_freq >= 0.15) & (fft_freq < 0.4)\n",
    "        \n",
    "        features = {\n",
    "            'total_power': total_power,\n",
    "            'vlf_power': np.sum(fft_vals[vlf_mask]**2) / total_power,\n",
    "            'lf_power': np.sum(fft_vals[lf_mask]**2) / total_power,\n",
    "            'hf_power': np.sum(fft_vals[hf_mask]**2) / total_power,\n",
    "            'dominant_freq': fft_freq[np.argmax(fft_vals)] if len(fft_vals) > 0 else 0,\n",
    "            'spectral_entropy': -np.sum((fft_vals/total_power) * np.log(fft_vals/total_power + 1e-8))\n",
    "        }\n",
    "        \n",
    "        features['lf_hf_ratio'] = features['lf_power'] / (features['hf_power'] + 1e-8)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # extract all features from a single ECG \n",
    "    def extract_all_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        features = {}\n",
    "        features.update(self.extract_statistical_features(x))\n",
    "        features.update(self.extract_temporal_features(x))\n",
    "        features.update(self.extract_frequency_features(x))\n",
    "        \n",
    "        if not self.feature_names:\n",
    "            self.feature_names = list(features.keys())\n",
    "        \n",
    "        return np.array(list(features.values()))\n",
    "    \n",
    "    # extract all features from multiple ECG signals\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        if X.ndim == 1:\n",
    "            return self.extract_all_features(X).reshape(1, -1)\n",
    "        elif X.ndim == 2:\n",
    "            return np.array([self.extract_all_features(x) for x in X])\n",
    "        else:\n",
    "            return np.array([self.extract_all_features(x[:, 0]) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features\n",
    "feature_extractor = ECGFeatureExtractor(config)\n",
    "\n",
    "X_train_features = feature_extractor.transform(X_train)\n",
    "X_test_features = feature_extractor.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_features.shape}\")\n",
    "print(f\"\\nExtracted features ({len(feature_extractor.feature_names)}):\")\n",
    "for i, name in enumerate(feature_extractor.feature_names):\n",
    "    print(f\"  {i+1}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train_features)\n",
    "X_test_scaled = feature_scaler.transform(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise Feature Distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "key_features = ['hr_estimate', 'rr_std', 'peak_height_mean', 'skewness', 'spectral_entropy', 'hrv']\n",
    "\n",
    "for ax, feat_name in zip(axes.flat, key_features):\n",
    "    if feat_name in feature_extractor.feature_names:\n",
    "        feat_idx = feature_extractor.feature_names.index(feat_name)\n",
    "        \n",
    "        normal_vals = X_train_features[y_train == 0, feat_idx]\n",
    "        anomaly_vals = X_train_features[y_train == 1, feat_idx]\n",
    "        \n",
    "        ax.hist(normal_vals, bins=30, alpha=0.6, label='Normal', color='blue')\n",
    "        ax.hist(anomaly_vals, bins=30, alpha=0.6, label='Anomaly', color='red')\n",
    "        ax.set_xlabel(feat_name)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions: Normal vs Anomalous ECGs', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X: np.ndarray) -> 'AnomalyDetector':\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest anomaly detection\n",
    "class IsolationForestDetector(AnomalyDetector):\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.model = IsolationForest(\n",
    "            n_estimators=config.if_n_estimators,\n",
    "            contamination=config.contamination,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.threshold = None\n",
    "        \n",
    "    def fit(self, X: np.ndarray) -> 'IsolationForestDetector':\n",
    "        self.model.fit(X)\n",
    "        scores = self.model.decision_function(X)\n",
    "        self.threshold = np.percentile(scores, 100 * self.config.contamination)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        preds = self.model.predict(X)\n",
    "        return np.where(preds == 1, 0, 1)\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        return -self.model.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Class SVM anomaly detection\n",
    "class OneClassSVMDetector(AnomalyDetector):\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.model = OneClassSVM(\n",
    "            kernel=config.svm_kernel,\n",
    "            nu=config.svm_nu,\n",
    "            gamma=config.svm_gamma\n",
    "        )\n",
    "        self.threshold = None\n",
    "        \n",
    "    def fit(self, X: np.ndarray) -> 'OneClassSVMDetector':\n",
    "        self.model.fit(X)\n",
    "        scores = self.model.decision_function(X)\n",
    "        self.threshold = np.percentile(scores, 100 * self.config.contamination)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        preds = self.model.predict(X)\n",
    "        return np.where(preds == 1, 0, 1)\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        return -self.model.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination (OCSVM and IF) of anomaly detectors\n",
    "class EnsembleAnomalyDetector(AnomalyDetector):\n",
    "    def __init__(self, detectors: List[AnomalyDetector], aggregation: str = 'mean'):\n",
    "        self.detectors = detectors\n",
    "        self.aggregation = aggregation\n",
    "        \n",
    "    def fit(self, X: np.ndarray) -> 'EnsembleAnomalyDetector':\n",
    "        for detector in self.detectors:\n",
    "            detector.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predictions = np.array([d.predict(X) for d in self.detectors])\n",
    "        if self.aggregation == 'voting':\n",
    "            return (np.mean(predictions, axis=0) >= 0.5).astype(int)\n",
    "        else:\n",
    "            scores = self.decision_function(X)\n",
    "            return (scores > 0).astype(int)\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = np.array([d.decision_function(X) for d in self.detectors])\n",
    "        return np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the detector\n",
    "def evaluate_detector(detector: AnomalyDetector, X_test: np.ndarray, y_test: np.ndarray, \n",
    "                      detector_name: str) -> Dict[str, float]:\n",
    "    y_pred = detector.predict(X_test)\n",
    "    scores = detector.decision_function(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'AUROC': roc_auc_score(y_test, scores),\n",
    "        'AUPRC': average_precision_score(y_test, scores),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{detector_name} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  TN={cm[0,0]}, FP={cm[0,1]}\")\n",
    "    print(f\"  FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the detectors\n",
    "if_detector = IsolationForestDetector(config)\n",
    "X_train_normal = X_train_scaled[y_train == 0]\n",
    "print(f\"Training Isolation Forest on {len(X_train_normal)} normal samples...\")\n",
    "if_detector.fit(X_train_normal)\n",
    "if_metrics = evaluate_detector(if_detector, X_test_scaled, y_test, \"Isolation Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_detector = OneClassSVMDetector(config)\n",
    "print(f\"Training One-Class SVM on {len(X_train_normal)} normal samples...\")\n",
    "svm_detector.fit(X_train_normal)\n",
    "svm_metrics = evaluate_detector(svm_detector, X_test_scaled, y_test, \"One-Class SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_detector = EnsembleAnomalyDetector(\n",
    "    detectors=[if_detector, svm_detector],\n",
    "    aggregation='mean'\n",
    ")\n",
    "ensemble_metrics = evaluate_detector(ensemble_detector, X_test_scaled, y_test, \"Ensemble (IF + OC-SVM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise ROC and PR curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for detector, name, color in [\n",
    "    (if_detector, 'Isolation Forest', 'blue'),\n",
    "    (svm_detector, 'One-Class SVM', 'red'),\n",
    "    (ensemble_detector, 'Ensemble', 'green')\n",
    "]:\n",
    "    scores = detector.decision_function(X_test_scaled)\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    auc = roc_auc_score(y_test, scores)\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves', fontsize=14)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for detector, name, color in [\n",
    "    (if_detector, 'Isolation Forest', 'blue'),\n",
    "    (svm_detector, 'One-Class SVM', 'red'),\n",
    "    (ensemble_detector, 'Ensemble', 'green')\n",
    "]:\n",
    "    scores = detector.decision_function(X_test_scaled)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, scores)\n",
    "    ap = average_precision_score(y_test, scores)\n",
    "    axes[1].plot(recall, precision, label=f'{name} (AP={ap:.3f})', color=color, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curves', fontsize=14)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('detection_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CounterfactualExplanation:\n",
    "    # original information\n",
    "    original_signal: np.ndarray\n",
    "    original_features: np.ndarray\n",
    "    original_prediction: int\n",
    "    original_score: float\n",
    "    \n",
    "    # counterfactual information\n",
    "    counterfactual_signal: np.ndarray\n",
    "    counterfactual_features: np.ndarray\n",
    "    counterfactual_prediction: int\n",
    "    counterfactual_score: float\n",
    "    \n",
    "    # difference analysis\n",
    "    feature_differences: Dict[str, float] = field(default_factory=dict)\n",
    "    signal_distance: float = 0.0\n",
    "    feature_distance: float = 0.0\n",
    "    \n",
    "    # quality metrics\n",
    "    validity: bool = False\n",
    "    proximity: float = 0.0\n",
    "    sparsity: int = 0\n",
    "    \n",
    "    # Top features that changed\n",
    "    top_changes: List[Tuple[str, float, float, float]] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def feature_changes(self) -> Dict[str, float]:\n",
    "        return self.feature_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neighbour counterfactual generator\n",
    "class NearestNeighbourCounterfactual:    \n",
    "    def __init__(self, config: PipelineConfig, detector: AnomalyDetector,\n",
    "                 feature_names: List[str]):\n",
    "        self.config = config\n",
    "        self.detector = detector\n",
    "        self.feature_names = feature_names\n",
    "        self.nn_model = None\n",
    "        self.normal_signals = None\n",
    "        self.normal_features = None\n",
    "\n",
    "    # Fit the nearest neighbour model on normal samples \n",
    "    def fit(self, X_signals: np.ndarray, X_features: np.ndarray, y: np.ndarray):\n",
    "        normal_mask = (y == 0)\n",
    "        self.normal_signals = X_signals[normal_mask]\n",
    "        self.normal_features = X_features[normal_mask]\n",
    "        \n",
    "        self.nn_model = NearestNeighbors(\n",
    "            n_neighbors=self.config.n_neighbors,\n",
    "            metric=self.config.distance_metric\n",
    "        )\n",
    "        self.nn_model.fit(self.normal_features)\n",
    "        \n",
    "        print(f\"Counterfactual generator fitted on {len(self.normal_signals)} normal samples\")\n",
    "\n",
    "    # Generate cfe for a single sample\n",
    "    def generate(self, signal: np.ndarray, features: np.ndarray) -> CounterfactualExplanation:\n",
    "        original_pred = self.detector.predict(features.reshape(1, -1))[0]\n",
    "        original_score = self.detector.decision_function(features.reshape(1, -1))[0]\n",
    "        \n",
    "        distances, indices = self.nn_model.kneighbors(features.reshape(1, -1))\n",
    "        \n",
    "        cf_idx = indices[0, 0]\n",
    "        cf_signal = self.normal_signals[cf_idx]\n",
    "        cf_features = self.normal_features[cf_idx]\n",
    "        \n",
    "        cf_pred = self.detector.predict(cf_features.reshape(1, -1))[0]\n",
    "        cf_score = self.detector.decision_function(cf_features.reshape(1, -1))[0]\n",
    "        \n",
    "        feature_diffs = {}\n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            feature_diffs[name] = cf_features[i] - features[i]\n",
    "        \n",
    "        signal_dist = euclidean(signal.flatten(), cf_signal.flatten())\n",
    "        feature_dist = distances[0, 0]\n",
    "        \n",
    "        validity = (original_pred == 1) and (cf_pred == 0)\n",
    "        \n",
    "        std_threshold = 1.0\n",
    "        sparsity = sum(1 for diff in feature_diffs.values() if abs(diff) > std_threshold)\n",
    "        \n",
    "        sorted_changes = sorted(\n",
    "            [(name, features[i], cf_features[i], diff) \n",
    "             for i, (name, diff) in enumerate(feature_diffs.items())],\n",
    "            key=lambda x: abs(x[3]),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return CounterfactualExplanation(\n",
    "            original_signal=signal,\n",
    "            original_features=features,\n",
    "            original_prediction=original_pred,\n",
    "            original_score=original_score,\n",
    "            counterfactual_signal=cf_signal,\n",
    "            counterfactual_features=cf_features,\n",
    "            counterfactual_prediction=cf_pred,\n",
    "            counterfactual_score=cf_score,\n",
    "            feature_differences=feature_diffs,\n",
    "            signal_distance=signal_dist,\n",
    "            feature_distance=feature_dist,\n",
    "            validity=validity,\n",
    "            proximity=feature_dist,\n",
    "            sparsity=sparsity,\n",
    "            top_changes=sorted_changes[:5]\n",
    "        )\n",
    "    \n",
    "    # generate cfe for multiple samples\n",
    "    def generate_batch(self, signals: np.ndarray, features: np.ndarray) -> List[CounterfactualExplanation]:\n",
    "        return [self.generate(s, f) for s, f in tqdm(zip(signals, features), total=len(signals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_counterfactual(cf: CounterfactualExplanation, title: str = \"\"):\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    ax1.plot(cf.original_signal, 'r-', label='Original (Anomaly)', linewidth=1.5, alpha=0.8)\n",
    "    ax1.plot(cf.counterfactual_signal, 'b-', label='Counterfactual (Normal)', linewidth=1.5, alpha=0.8)\n",
    "    ax1.set_xlabel('Time (samples)')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax1.set_title('ECG Signal Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    diff = cf.original_signal - cf.counterfactual_signal\n",
    "    ax2.fill_between(range(len(diff)), diff, alpha=0.5, color='purple')\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    ax2.set_xlabel('Time (samples)')\n",
    "    ax2.set_ylabel('Difference')\n",
    "    ax2.set_title('Signal Difference (Original - Counterfactual)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    top_n = min(5, len(cf.top_changes))\n",
    "    names = [c[0] for c in cf.top_changes[:top_n]]\n",
    "    orig_vals = [c[1] for c in cf.top_changes[:top_n]]\n",
    "    cf_vals = [c[2] for c in cf.top_changes[:top_n]]\n",
    "    \n",
    "    x = np.arange(top_n)\n",
    "    width = 0.35\n",
    "    ax3.bar(x - width/2, orig_vals, width, label='Original', color='red', alpha=0.7)\n",
    "    ax3.bar(x + width/2, cf_vals, width, label='Counterfactual', color='blue', alpha=0.7)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Feature Value (scaled)')\n",
    "    ax3.set_title('Top Feature Changes')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    COUNTERFACTUAL QUALITY METRICS\n",
    "    {'='*40}\n",
    "    \n",
    "    Validity: {'✓ VALID' if cf.validity else '✗ INVALID'}\n",
    "    (Prediction flipped from anomaly to normal)\n",
    "    \n",
    "    Proximity: {cf.proximity:.4f}\n",
    "    (Distance to counterfactual in feature space)\n",
    "    \n",
    "    Sparsity: {cf.sparsity} features changed significantly\n",
    "    (Features with |diff| > 1 std)\n",
    "    \n",
    "    Original Score: {cf.original_score:.4f}\n",
    "    Counterfactual Score: {cf.counterfactual_score:.4f}\n",
    "    \n",
    "    TOP CHANGES:\n",
    "    \"\"\"\n",
    "    \n",
    "    for name, orig, cf_val, diff in cf.top_changes[:3]:\n",
    "        direction = \"↑\" if diff > 0 else \"↓\"\n",
    "        metrics_text += f\"\\n    • {name}: {orig:.2f} → {cf_val:.2f} ({direction} {abs(diff):.2f})\"\n",
    "    \n",
    "    ax4.text(0.1, 0.9, metrics_text, transform=ax4.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle(title or 'Counterfactual Explanation', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise cf generator\n",
    "cf_generator = NearestNeighbourCounterfactual(\n",
    "    config=config,\n",
    "    detector=ensemble_detector,\n",
    "    feature_names=feature_extractor.feature_names\n",
    ")\n",
    "\n",
    "cf_generator.fit(X_train, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate counterfactuals for anomalous samples\n",
    "anomaly_test_idx = np.where(y_test == 1)[0]\n",
    "print(f\"Generating counterfactuals for {len(anomaly_test_idx)} anomalous samples...\")\n",
    "\n",
    "n_examples = min(10, len(anomaly_test_idx))\n",
    "sample_indices = anomaly_test_idx[:n_examples]\n",
    "\n",
    "counterfactuals = []\n",
    "for idx in sample_indices:\n",
    "    cf = cf_generator.generate(X_test[idx], X_test_scaled[idx])\n",
    "    counterfactuals.append(cf)\n",
    "\n",
    "print(f\"\\nCounterfactual Quality Summary:\")\n",
    "print(f\"  Validity Rate: {sum(cf.validity for cf in counterfactuals) / len(counterfactuals):.2%}\")\n",
    "print(f\"  Mean Proximity: {np.mean([cf.proximity for cf in counterfactuals]):.4f}\")\n",
    "print(f\"  Mean Sparsity: {np.mean([cf.sparsity for cf in counterfactuals]):.1f} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a few example counterfactuals\n",
    "for i, cf in enumerate(counterfactuals[:3]):\n",
    "    fig = visualize_counterfactual(cf, f\"Counterfactual Example {i+1}\")\n",
    "    plt.savefig(f'counterfactual_example_{i+1}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Clinical Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClinicalExplanation:\n",
    "    flag_reason: str              # why did the AI flag this ECG\n",
    "    key_differences: List[str]    # what is the main differences from normal (3 items)\n",
    "    interpretation_note: str      # what is the clinical context for these differences\n",
    "    confidence_statement: str     # how reliable is this assessment\n",
    "    \n",
    "    raw_response: str = \"\"\n",
    "    model_used: str = \"\"\n",
    "    tokens_used: int = 0\n",
    "    generation_time: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMExplanationGenerator:\n",
    "    # clinical context for ECG features\n",
    "    FEATURE_CLINICAL_CONTEXT = {\n",
    "        'hr_estimate': ('Heart rate', 'BPM', 'Normal resting: 60-100 BPM'),\n",
    "        'rr_mean': ('R-R interval (time between beats)', 'samples', 'Regularity indicates rhythm stability'),\n",
    "        'rr_std': ('R-R variability', 'samples', 'Higher values suggest irregular rhythm'),\n",
    "        'rr_min': ('Minimum R-R interval', 'samples', 'Very short intervals may indicate ectopic beats'),\n",
    "        'rr_max': ('Maximum R-R interval', 'samples', 'Long intervals may indicate pauses or blocks'),\n",
    "        'hrv': ('Heart rate variability', 'ratio', 'Reflects autonomic nervous system function'),\n",
    "        'n_peaks': ('Number of detected heartbeats', 'count', 'Should match expected for recording duration'),\n",
    "        'peak_height_mean': ('Average R-wave amplitude', 'normalized units', 'Changes may reflect conduction abnormalities'),\n",
    "        'peak_height_std': ('R-wave amplitude variability', 'normalized units', 'Consistency of ventricular depolarization'),\n",
    "        'mean': ('Signal mean', 'units', 'Baseline level of the ECG'),\n",
    "        'std': ('Signal standard deviation', 'units', 'Overall variability in the signal'),\n",
    "        'var': ('Signal variance', 'units', 'Squared variability measure'),\n",
    "        'min': ('Signal minimum', 'units', 'Lowest point in the ECG'),\n",
    "        'max': ('Signal maximum', 'units', 'Highest point (typically R-peak)'),\n",
    "        'range': ('Signal range', 'units', 'Difference between max and min'),\n",
    "        'median': ('Signal median', 'units', 'Central value of the signal'),\n",
    "        'skewness': ('Waveform asymmetry', 'units', 'May indicate ST segment changes'),\n",
    "        'kurtosis': ('Waveform peakedness', 'units', 'Related to QRS morphology'),\n",
    "        'rms': ('Root mean square', 'units', 'Signal energy measure'),\n",
    "        'zero_crossings': ('Zero crossings', 'count', 'Frequency-related measure'),\n",
    "        'total_power': ('Total spectral power', 'units', 'Overall signal energy'),\n",
    "        'vlf_power': ('Very low frequency power', 'ratio', 'Long-term regulatory mechanisms'),\n",
    "        'lf_power': ('Low frequency power', 'ratio', 'Sympathetic and parasympathetic activity'),\n",
    "        'hf_power': ('High frequency power', 'ratio', 'Parasympathetic (vagal) activity'),\n",
    "        'dominant_freq': ('Dominant frequency', 'Hz', 'Primary frequency component'),\n",
    "        'spectral_entropy': ('Signal complexity', 'units', 'Lower values suggest more regular patterns'),\n",
    "        'lf_hf_ratio': ('LF/HF ratio', 'ratio', 'Sympathovagal balance indicator'),\n",
    "    }\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a clinical decision support assistant that explains ECG anomaly detection results. Your role is NOT to diagnose or recommend actions—clinicians make those decisions. Your role is to explain WHY an AI system flagged an ECG as potentially abnormal.\n",
    "\n",
    "You translate technical counterfactual explanations into clear language. A counterfactual explanation works by answering: \"What would need to be different about this ECG for it to appear normal?\"\n",
    "\n",
    "AUDIENCE: Your explanations must be understood by cardiologists, general practitioners, and cardiac nurses—use precise but accessible language.\n",
    "\n",
    "CRITICAL CONSTRAINTS:\n",
    "- Never diagnose conditions or recommend clinical actions\n",
    "- Never use phrases like \"you should\" or \"this indicates [diagnosis]\"\n",
    "- Always frame explanations as \"the AI flagged this because...\" not \"this patient has...\"\n",
    "- Keep the total explanation under 150 words\n",
    "- Be honest about uncertainty—these are computational patterns, not clinical diagnoses\n",
    "\n",
    "OUTPUT FORMAT: Return ONLY valid JSON with no additional text, markdown or explanation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self._openai_client = None\n",
    "        self._local_generator = None\n",
    "        self._local_tokenizer = None\n",
    "\n",
    "    \n",
    "    def _format_feature_changes(self, cf: CounterfactualExplanation, top_n: int = 5) -> str:\n",
    "        lines = []\n",
    "        \n",
    "        for name, orig_val, cf_val, diff in cf.top_changes[:top_n]:\n",
    "            context = self.FEATURE_CLINICAL_CONTEXT.get(name, (name, 'units', ''))\n",
    "            direction = \"higher\" if diff > 0 else \"lower\"\n",
    "            abs_diff = abs(diff)\n",
    "            \n",
    "            line = f\"- {context[0]}: This ECG is {abs_diff:.2f} {context[1]} {direction} than the normal reference. {context[2]}\"\n",
    "            lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def _build_user_prompt(self, cf: CounterfactualExplanation) -> str:\n",
    "        feature_changes_text = self._format_feature_changes(cf)\n",
    "        \n",
    "        # simple confidence indicator based on the proximity\n",
    "        if cf.proximity < 2.0:\n",
    "            confidence_hint = \"relatively close to normal patterns\"\n",
    "        elif cf.proximity < 4.0:\n",
    "            confidence_hint = \"moderately different from normal patterns\"\n",
    "        else:\n",
    "            confidence_hint = \"substantially different from normal patterns\"\n",
    "        \n",
    "        return f\"\"\"An ECG was flagged as potentially abnormal by an anomaly detection system. Below are the key differences between this ECG and the nearest \"normal\" reference ECG found by the system.\n",
    "\n",
    "DETECTION INFO:\n",
    "- Anomaly Score: {cf.original_score:.3f} (higher = more anomalous)\n",
    "- Distance from nearest normal: {cf.proximity:.3f} ({confidence_hint})\n",
    "\n",
    "FEATURE DIFFERENCES (what would need to change for this ECG to appear normal):\n",
    "{feature_changes_text}\n",
    "\n",
    "Translate this into a clinical explanation. Return ONLY valid JSON with exactly these fields:\n",
    "\n",
    "{{\n",
    "  \"flag_reason\": \"One to two sentences explaining in plain English why the AI flagged this ECG as abnormal. Start with 'This ECG was flagged because...'\",\n",
    "  \n",
    "  \"key_differences\": [\n",
    "    \"First key difference between this ECG and normal reference (one sentence)\",\n",
    "    \"Second key difference (one sentence)\", \n",
    "    \"Third key difference if relevant, otherwise omit this item (one sentence)\"\n",
    "  ],\n",
    "  \n",
    "  \"interpretation_note\": \"One sentence providing clinical context for these differences, accessible to non-specialists. Frame as 'These patterns may be associated with...' not as a diagnosis.\",\n",
    "  \n",
    "  \"confidence_statement\": \"One sentence about the reliability of this flag based on how different this ECG is from normal references.\"\n",
    "}}\"\"\"\n",
    "\n",
    "    def _parse_llm_response(self, response_text: str) -> dict:\n",
    "        text = response_text.strip()\n",
    "        \n",
    "        # remove markdown blocks\n",
    "        if text.startswith(\"```\"):\n",
    "            lines = text.split(\"\\n\")\n",
    "            if lines[0].startswith(\"```\"):\n",
    "                lines = lines[1:]\n",
    "            if lines and lines[-1].strip() == \"```\":\n",
    "                lines = lines[:-1]\n",
    "            text = \"\\n\".join(lines)\n",
    "        \n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}')\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            try:\n",
    "                return json.loads(text[start:end + 1])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        raise ValueError(f\"Could not parse JSON from LLM response: {text[:200]}...\")\n",
    "\n",
    "    def _validate_response(self, data: dict) -> dict:\n",
    "        required_fields = ['flag_reason', 'key_differences', 'interpretation_note', 'confidence_statement']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                raise ValueError(f\"Missing required field: {field}\")\n",
    "        \n",
    "        if not isinstance(data['key_differences'], list):\n",
    "            data['key_differences'] = [str(data['key_differences'])]\n",
    "        \n",
    "        data['key_differences'] = data['key_differences'][:3]\n",
    "        \n",
    "        for field in ['flag_reason', 'interpretation_note', 'confidence_statement']:\n",
    "            data[field] = str(data[field])\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def _generate_with_openai(self, cf: CounterfactualExplanation) -> Tuple[str, int]:\n",
    "        if self._openai_client is None:\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "            except ImportError:\n",
    "                raise ImportError(\"openai package not installed. Run: pip install openai\")\n",
    "            \n",
    "            api_key = os.getenv(self.config.llm_api_key_env)\n",
    "            if not api_key:\n",
    "                raise ValueError(f\"OpenAI API key not found in environment variable: {self.config.llm_api_key_env}\")\n",
    "            \n",
    "            self._openai_client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        user_prompt = self._build_user_prompt(cf)\n",
    "        \n",
    "        response = self._openai_client.chat.completions.create(\n",
    "            model=self.config.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=self.config.llm_temperature,\n",
    "            max_tokens=self.config.llm_max_tokens,\n",
    "            response_format={\"type\": \"json_object\"}  \n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        tokens_used = response.usage.total_tokens if response.usage else 0\n",
    "        \n",
    "        return response_text, tokens_used\n",
    "\n",
    "    # generate explanation for cf with structured fields\n",
    "    def generate(self, cf: CounterfactualExplanation) -> ClinicalExplanation:\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        backend = self.config.llm_backend.lower()\n",
    "        \n",
    "        try:\n",
    "            if backend == 'openai':\n",
    "                response_text, tokens_used = self._generate_with_openai(cf)\n",
    "            elif backend == 'local':\n",
    "                response_text, tokens_used = self._generate_with_local(cf)\n",
    "            elif backend == 'gemini':\n",
    "                response_text, tokens_used = self._generate_with_gemini(cf)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown LLM backend: {backend}\")\n",
    "            \n",
    "            data = self._parse_llm_response(response_text)\n",
    "            data = self._validate_response(data)\n",
    "            \n",
    "            generation_time = time.perf_counter() - start_time\n",
    "            \n",
    "            return ClinicalExplanation(\n",
    "                flag_reason=data['flag_reason'],\n",
    "                key_differences=data['key_differences'],\n",
    "                interpretation_note=data['interpretation_note'],\n",
    "                confidence_statement=data['confidence_statement'],\n",
    "                raw_response=response_text,\n",
    "                model_used=self.config.llm_model if backend == 'openai' else self.config.llm_local_model,\n",
    "                tokens_used=tokens_used,\n",
    "                generation_time=generation_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            generation_time = time.perf_counter() - start_time\n",
    "            \n",
    "            top_features = [name for name, _, _, _ in cf.top_changes[:3]]\n",
    "            \n",
    "            return ClinicalExplanation(\n",
    "                flag_reason=f\"\",\n",
    "                key_differences=f\"\",\n",
    "                interpretation_note=f\"\",\n",
    "                confidence_statement=f\"\",\n",
    "                raw_response=f\"\",\n",
    "                model_used=f\"\",\n",
    "                tokens_used=0,\n",
    "                generation_time=generation_time\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_clinical_explanation(cf: CounterfactualExplanation, explanation: ClinicalExplanation):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLINICAL EXPLANATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n WHY WAS THIS FLAGGED?\")\n",
    "    print(\"-\"*50)\n",
    "    print(explanation.flag_reason)\n",
    "    \n",
    "    print(f\"\\n KEY DIFFERENCES FROM NORMAL\")\n",
    "    print(\"-\"*50)\n",
    "    for i, diff in enumerate(explanation.key_differences, 1):\n",
    "        print(f\"  {i}. {diff}\")\n",
    "    \n",
    "    print(f\"\\n CLINICAL CONTEXT\")\n",
    "    print(\"-\"*50)\n",
    "    print(explanation.interpretation_note)\n",
    "    \n",
    "    print(f\"\\nCONFIDENCE\")\n",
    "    print(\"-\"*50)\n",
    "    print(explanation.confidence_statement)\n",
    "    \n",
    "    print(f\"\\n METADATA\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"  Model: {explanation.model_used}\")\n",
    "    print(f\"  Tokens: {explanation.tokens_used}\")\n",
    "    print(f\"  Time: {explanation.generation_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_generator = LLMExplanationGenerator(config)\n",
    "api_key = os.getenv(config.llm_api_key_env)\n",
    "if not api_key:\n",
    "    print(f\"Warning: {config.llm_api_key_env} not set\")\n",
    "    config.llm_backend = 'local'\n",
    "else:\n",
    "    print(f\"API key found. Using {config.llm_model}\")\n",
    "\n",
    "print(\"Generating clinical explanations...\\n\")\n",
    "\n",
    "for i, cf in enumerate(counterfactuals[:3]):\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# EXAMPLE {i+1}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    explanation = llm_generator.generate(cf)\n",
    "    display_clinical_explanation(cf, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable ECG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineResult:\n",
    "    ecg_signal: np.ndarray\n",
    "    ecg_id: str\n",
    "    \n",
    "    #1. detection\n",
    "    is_anomaly: bool\n",
    "    anomaly_score: float\n",
    "    detection_confidence: float\n",
    "    \n",
    "    #2. counterfactual \n",
    "    counterfactual: Optional[CounterfactualExplanation]\n",
    "    \n",
    "    #3. clinical explanation\n",
    "    clinical_explanation: Optional[ClinicalExplanation]\n",
    "    \n",
    "    processing_time: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableECGPipeline:\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.preprocessor = ECGPreprocessor(config)\n",
    "        self.feature_extractor = ECGFeatureExtractor(config)\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.if_detector = IsolationForestDetector(config)\n",
    "        self.svm_detector = OneClassSVMDetector(config)\n",
    "        self.ensemble_detector = EnsembleAnomalyDetector(\n",
    "            detectors=[self.if_detector, self.svm_detector],\n",
    "            aggregation='mean'\n",
    "        )\n",
    "        self.cf_generator: Optional[NearestNeighbourCounterfactual] = None\n",
    "        self.llm_generator = LLMExplanationGenerator(config)\n",
    "        self.score_scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X_signals: np.ndarray, y: np.ndarray) -> \"ExplainableECGPipeline\":\n",
    "        X_processed = self.preprocessor.preprocess(X_signals)\n",
    "        X_features = self.feature_extractor.transform(X_processed)\n",
    "        self.feature_scaler.fit(X_features)\n",
    "        X_scaled = self.feature_scaler.transform(X_features)\n",
    "\n",
    "        normal_mask = (y == 0)\n",
    "        X_normal = X_scaled[normal_mask]\n",
    "        self.if_detector.fit(X_normal)\n",
    "        self.svm_detector.fit(X_normal)\n",
    "        self.ensemble_detector.fit(X_normal)\n",
    "\n",
    "        scores = self.ensemble_detector.decision_function(X_scaled).reshape(-1, 1)\n",
    "        self.score_scaler.fit(scores)\n",
    "\n",
    "        self.cf_generator = NearestNeighbourCounterfactual(\n",
    "            config=self.config,\n",
    "            detector=self.ensemble_detector,\n",
    "            feature_names=self.feature_extractor.feature_names\n",
    "        )\n",
    "        self.cf_generator.fit(X_processed, X_scaled, y)\n",
    "        return self\n",
    "\n",
    "    def _confidence_from_score(self, score: float) -> float:\n",
    "        scaled = self.score_scaler.transform([[score]])[0][0]\n",
    "        return float(np.clip(scaled, 0.0, 1.0))\n",
    "\n",
    "    def predict(self, ecg_signal: np.ndarray, ecg_id: str, \n",
    "                generate_explanation: bool = True) -> PipelineResult:\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        X_processed = self.preprocessor.preprocess(ecg_signal)\n",
    "        X_features = self.feature_extractor.transform(X_processed)\n",
    "        X_scaled = self.feature_scaler.transform(X_features)\n",
    "\n",
    "        is_anomaly = bool(self.ensemble_detector.predict(X_scaled)[0])\n",
    "        anomaly_score = float(self.ensemble_detector.decision_function(X_scaled)[0])\n",
    "        detection_confidence = self._confidence_from_score(anomaly_score)\n",
    "\n",
    "        counterfactual = None\n",
    "        clinical_explanation = None\n",
    "        \n",
    "        if is_anomaly and self.cf_generator is not None:\n",
    "            counterfactual = self.cf_generator.generate(X_processed, X_scaled[0])\n",
    "            \n",
    "            if generate_explanation:\n",
    "                clinical_explanation = self.llm_generator.generate(counterfactual)\n",
    "\n",
    "        processing_time = time.perf_counter() - start_time\n",
    "\n",
    "        return PipelineResult(\n",
    "            ecg_signal=ecg_signal,\n",
    "            ecg_id=ecg_id,\n",
    "            is_anomaly=is_anomaly,\n",
    "            anomaly_score=anomaly_score,\n",
    "            detection_confidence=detection_confidence,\n",
    "            counterfactual=counterfactual,\n",
    "            clinical_explanation=clinical_explanation,\n",
    "            processing_time=processing_time\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ExplainableECGPipeline(config)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING PIPELINE ON TEST SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_indices = [0, 1, 2]\n",
    "\n",
    "for idx in test_indices:\n",
    "    result = pipeline.predict(X_test[idx], f\"TEST_{idx:04d}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ECG ID: {result.ecg_id}\")\n",
    "    print(f\"Ground Truth: {'Anomaly' if y_test[idx] == 1 else 'Normal'}\")\n",
    "    print(f\"Prediction: {'Anomaly' if result.is_anomaly else 'Normal'}\")\n",
    "    print(f\"Anomaly Score: {result.anomaly_score:.4f}\")\n",
    "    print(f\"Detection Confidence: {result.detection_confidence:.2%}\")\n",
    "    print(f\"Processing Time: {result.processing_time:.3f}s\")\n",
    "    \n",
    "    if result.clinical_explanation:\n",
    "        print(f\"\\n--- Clinical Explanation ---\")\n",
    "        print(f\"Flag Reason: {result.clinical_explanation.flag_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(pipeline: ExplainableECGPipeline, \n",
    "                             X_test: np.ndarray, y_test: np.ndarray) -> Dict:\n",
    "    results = {}\n",
    "    \n",
    "    X_processed = pipeline.preprocessor.preprocess(X_test)\n",
    "    X_features = pipeline.feature_extractor.transform(X_processed)\n",
    "    X_scaled = pipeline.feature_scaler.transform(X_features)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETECTION LAYER EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_pred = pipeline.ensemble_detector.predict(X_scaled)\n",
    "    scores = pipeline.ensemble_detector.decision_function(X_scaled)\n",
    "    \n",
    "    results['detection'] = {\n",
    "        'auroc': roc_auc_score(y_test, scores),\n",
    "        'auprc': average_precision_score(y_test, scores),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    for name, value in results['detection'].items():\n",
    "        print(f\"  {name.upper()}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COUNTERFACTUAL LAYER EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    anomaly_indices = np.where(y_test == 1)[0]\n",
    "    counterfactuals = []\n",
    "    \n",
    "    for idx in tqdm(anomaly_indices[:50], desc=\"Generating counterfactuals\"):\n",
    "        cf = pipeline.cf_generator.generate(X_processed[idx], X_scaled[idx])\n",
    "        counterfactuals.append(cf)\n",
    "    \n",
    "    results['counterfactual'] = {\n",
    "        'validity_rate': np.mean([cf.validity for cf in counterfactuals]),\n",
    "        'mean_proximity': np.mean([cf.proximity for cf in counterfactuals]),\n",
    "        'std_proximity': np.std([cf.proximity for cf in counterfactuals]),\n",
    "        'mean_sparsity': np.mean([cf.sparsity for cf in counterfactuals])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMetrics (on {len(counterfactuals)} samples):\")\n",
    "    print(f\"  Validity Rate: {results['counterfactual']['validity_rate']:.2%}\")\n",
    "    print(f\"  Mean Proximity: {results['counterfactual']['mean_proximity']:.4f} ± {results['counterfactual']['std_proximity']:.4f}\")\n",
    "    print(f\"  Mean Sparsity: {results['counterfactual']['mean_sparsity']:.1f} features\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = comprehensive_evaluation(pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise evaluation results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "detection_metrics = evaluation_results['detection']\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(detection_metrics.keys(), detection_metrics.values(), color='steelblue')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Detection Performance')\n",
    "ax1.axhline(y=0.75, color='r', linestyle='--', label='Target (0.75)')\n",
    "ax1.legend()\n",
    "for bar, val in zip(bars, detection_metrics.values()):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "cf_metrics = evaluation_results['counterfactual']\n",
    "ax2 = axes[1]\n",
    "cf_labels = ['Validity\\nRate', 'Mean\\nProximity', 'Mean\\nSparsity']\n",
    "cf_values = [cf_metrics['validity_rate'], \n",
    "             cf_metrics['mean_proximity'] / 5,\n",
    "             cf_metrics['mean_sparsity'] / 10]\n",
    "bars = ax2.bar(cf_labels, cf_values, color='seagreen')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_ylabel('Normalized Score')\n",
    "ax2.set_title('Counterfactual Quality')\n",
    "ax2.axhline(y=0.80, color='r', linestyle='--', label='Target (0.80)')\n",
    "ax2.legend()\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "PIPELINE EVALUATION SUMMARY\n",
    "{'='*35}\n",
    "\n",
    "DETECTION LAYER\n",
    "  AUROC: {detection_metrics['auroc']:.4f}\n",
    "  AUPRC: {detection_metrics['auprc']:.4f}\n",
    "  F1 Score: {detection_metrics['f1']:.4f}\n",
    "\n",
    "COUNTERFACTUAL LAYER\n",
    "  Validity Rate: {cf_metrics['validity_rate']:.2%}\n",
    "  Proximity: {cf_metrics['mean_proximity']:.4f}\n",
    "  Sparsity: {cf_metrics['mean_sparsity']:.1f}\n",
    "\n",
    "OVERALL STATUS\n",
    "  Detection: {'✓ PASS' if detection_metrics['auroc'] >= 0.75 else '✗ NEEDS WORK'}\n",
    "  Counterfactual: {'✓ PASS' if cf_metrics['validity_rate'] >= 0.80 else '✗ NEEDS WORK'}\n",
    "\"\"\"\n",
    "ax3.text(0.1, 0.95, summary_text, transform=ax3.transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pipeline_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Clinical Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clinical_report(result: PipelineResult) -> str:\n",
    "    report = f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║                       ECG ANALYSIS REPORT                            ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║ ECG ID: {result.ecg_id:<60} ║\n",
    "║ Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'):<52} ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║ DETECTION RESULT                                                      ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║ Classification: {'ABNORMAL' if result.is_anomaly else 'NORMAL':<57} ║\n",
    "║ Anomaly Score: {result.anomaly_score:>8.4f}                                          ║\n",
    "║ Confidence: {result.detection_confidence:>6.1%}                                             ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "\"\"\"\n",
    "    \n",
    "    if result.is_anomaly and result.clinical_explanation:\n",
    "        exp = result.clinical_explanation\n",
    "        key_diff_text = \"\\n\".join([f\"  • {diff}\" for diff in exp.key_differences])\n",
    "        \n",
    "        report += f\"\"\"\n",
    "║ CLINICAL INTERPRETATION                                              ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "\n",
    "WHY THIS ECG WAS FLAGGED:\n",
    "{exp.flag_reason}\n",
    "\n",
    "KEY DIFFERENCES FROM NORMAL:\n",
    "{key_diff_text}\n",
    "\n",
    "CLINICAL CONTEXT:\n",
    "{exp.interpretation_note}\n",
    "\n",
    "CONFIDENCE ASSESSMENT:\n",
    "{exp.confidence_statement}\n",
    "\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║ This is AI-assisted analysis. Clinical judgment is required.      ║\n",
    "║ This explanation is for decision support, not diagnosis.             ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "    else:\n",
    "        report += \"\"\"\n",
    "║ No significant abnormalities detected by the AI system.              ║\n",
    "║ Standard clinical follow-up as indicated.                            ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"#\" + \" \"*22 + \"CLINICAL CASE STUDIES\" + \" \"*23 + \"#\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "# find the correctly detected anomalies\n",
    "detected_anomalies = [\n",
    "    i for i in range(len(X_test)) \n",
    "    if y_test[i] == 1 and pipeline.ensemble_detector.predict(\n",
    "        pipeline.feature_scaler.transform(\n",
    "            pipeline.feature_extractor.transform(\n",
    "                pipeline.preprocessor.preprocess(X_test[i:i+1])\n",
    "            )\n",
    "        )\n",
    "    )[0] == 1\n",
    "]\n",
    "\n",
    "print(f\"\\nFound {len(detected_anomalies)} correctly detected anomalies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(detected_anomalies[:2]):\n",
    "    result = pipeline.predict(X_test[idx], f\"CASE_{i+1:03d}\")\n",
    "    report = generate_clinical_report(result)\n",
    "    print(report)\n",
    "    \n",
    "    if result.counterfactual:\n",
    "        fig = visualize_counterfactual(result.counterfactual, f\"Case Study {i+1}\")\n",
    "        plt.savefig(f'case_study_{i+1}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_jsonable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): _to_jsonable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [_to_jsonable(v) for v in obj]\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return [_to_jsonable(v) for v in obj.tolist()]\n",
    "    if isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_explanations_for_evaluation(pipeline: ExplainableECGPipeline,\n",
    "                                       X_test: np.ndarray, \n",
    "                                       y_test: np.ndarray,\n",
    "                                       n_samples: int = 20,\n",
    "                                       output_path: str = \"explanations_for_evaluation.json\") -> List[Dict]:\n",
    "    anomaly_indices = np.where(y_test == 1)[0]\n",
    "    \n",
    "    if len(anomaly_indices) < n_samples:\n",
    "        n_samples = len(anomaly_indices)\n",
    "    \n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    selected_indices = np.random.choice(anomaly_indices, size=n_samples, replace=False)\n",
    "    \n",
    "    evaluation_data = []\n",
    "    \n",
    "    print(f\"Generating {n_samples} explanations for evaluation...\")\n",
    "    \n",
    "    for i, idx in enumerate(tqdm(selected_indices)):\n",
    "        result = pipeline.predict(X_test[idx], f\"EVAL_{i:03d}\")\n",
    "        \n",
    "        if result.clinical_explanation and result.counterfactual:\n",
    "            exp = result.clinical_explanation\n",
    "            cf = result.counterfactual\n",
    "            \n",
    "            record = {\n",
    "                \"id\": result.ecg_id,\n",
    "                \"sample_index\": int(idx),\n",
    "                \"ground_truth\": \"anomaly\",\n",
    "                \"predicted\": \"anomaly\" if result.is_anomaly else \"normal\",\n",
    "                \"anomaly_score\": round(result.anomaly_score, 4),\n",
    "                \"detection_confidence\": round(result.detection_confidence, 4),\n",
    "                \n",
    "                \"explanation\": {\n",
    "                    \"flag_reason\": exp.flag_reason,\n",
    "                    \"key_differences\": exp.key_differences,\n",
    "                    \"interpretation_note\": exp.interpretation_note,\n",
    "                    \"confidence_statement\": exp.confidence_statement\n",
    "                },\n",
    "                \n",
    "                # cf quality metrics\n",
    "                \"counterfactual_metrics\": {\n",
    "                    \"validity\": cf.validity,\n",
    "                    \"proximity\": round(cf.proximity, 4),\n",
    "                    \"sparsity\": cf.sparsity\n",
    "                },\n",
    "                \n",
    "                # top feature changes \n",
    "                \"top_feature_changes\": [\n",
    "                    {\n",
    "                        \"feature\": name,\n",
    "                        \"original_value\": round(orig, 4),\n",
    "                        \"counterfactual_value\": round(cf_val, 4),\n",
    "                        \"difference\": round(diff, 4)\n",
    "                    }\n",
    "                    for name, orig, cf_val, diff in cf.top_changes[:5]\n",
    "                ],\n",
    "                \n",
    "                \"model_used\": exp.model_used,\n",
    "                \"generation_time\": round(exp.generation_time, 3)\n",
    "            }\n",
    "            \n",
    "            evaluation_data.append(_to_jsonable(record))\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(evaluation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nExported {len(evaluation_data)} explanations to: {output_path}\")\n",
    "    \n",
    "    return evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_export = export_explanations_for_evaluation(\n",
    "    pipeline, X_test, y_test, \n",
    "    n_samples=min(20, len(np.where(y_test == 1)[0])),\n",
    "    output_path=\"explanations_for_evaluation.json\"\n",
    ")\n",
    "\n",
    "print(\"\\nSample evaluation record:\")\n",
    "print(json.dumps(evaluation_export[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pipeline_components = {\n",
    "    'config': config,\n",
    "    'feature_scaler': pipeline.feature_scaler,\n",
    "    'feature_names': pipeline.feature_extractor.feature_names,\n",
    "    'if_detector': pipeline.if_detector,\n",
    "    'svm_detector': pipeline.svm_detector,\n",
    "}\n",
    "\n",
    "with open('pipeline_components.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline_components, f)\n",
    "\n",
    "print(\"Pipeline components saved to 'pipeline_components.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
